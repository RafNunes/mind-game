Playing The Chess Evaluator Against Itself

Once we had a chess evaluator that could play basic chess against another human the project became much more open ended. The entire focus was now just in improving the chess evaluator. However this is a very difficult thing to measure in a scientific manner so a new testing method was put in place. Now during the implementation two versions of the chess AI were kept. An older version and a newer version. After any significant changes the newer version was played against the older version as white and as black. If the new version won both games the effects had been positive and the AI was also balanced(i.e. the AI could play as well in White as it could in Black). Otherwise the effects were considered negative and adjusted until the updated AI could achieve victory. 

While this seems fine on paper there was a few flaws with this approach. Firstly, games did not always end. Due to the fact the rule that states the same position must only be played three times had not been implemented, games would sometimes reach an infinite loop where the AIs would play the same two moves continuously. As well as this slight changes were hard to recognise. Adding minor values to be evaluated had relatively little effect so the game may well have played out in the exact same way it had previously. Finally this method was very time consuming, games could often last for fifteen minutes and play quality was often hard to understand. A way round this was for members of Team H to play against the evaluator personally but the time this would take made it an unreasonable way forward.
