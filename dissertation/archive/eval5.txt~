Formal Evaluation of project

Once the product was completed it was time to a final evaluation in order to see if we had met our requirements set out in the original project specification which stated "The aim of this project is to develop a program that plays one of the more complex mind games at a standard good enough to beat casual players". \cite{Spec} To find out if the project realised these requirements we set out to do a formal evaluation process involving casual chess players. 

Following a briefing from a Team H member and after signing the ethics agreement (Found in Appendix A) the tester would select a difficulty and play a single game of chess against our AI. After completing the game the tester was asked to fill out a questionnaire (Appendix B). This questionnaire asks some brief questions about age and gender then goes on to discuss the program itself.

During the evaluation the chess AI won 8, lost 2 and drew 2 games. While on the face of it this may seem like a strong set of results for the AI and a solid achievement of our goal there is still many things to take into consideration. The first is the quality of the players. Of the twelve testers only five of them put their level above beginner, with all five claiming to be Amateur. Out of the three difficulty levels no person picked Challenging while the majority picked easy. Secondly, chess is a very difficult game to get people to play properly. When playing chess you really need no distractions and to spend some time considering moves. Some players, probably due to being watched by a member of the team felt stressed and did not think out moves properly. One such example of this was a player losing in seven moves to our AI. Obviously this would not have happened if the player was thinking about the moves properly. Also the amount of time consumed by a game would have been an issue. Some games would drag on and testers would lose interest and start making moves with the intent of losing. 

As part of the evaluation we asked three open ended questions to allow testers to give detailed feedback. Question one asked for further comments about the difficulty level. The majority of users left this blank however some users left insightful comments. One user complained that the engine needlessly threw away a pawn that was on the sixth rank, and did not retaliate. This same user then went on to win the game. Even at after a review of the moves it is not obvious why the engine did this as it could have became a queen at a later date. Another comment from several users was the queen coming out early in an attempt to quickly win the game. This often backfired against the more experienced players who spent developing pieces to push the queen back. We believe this was caused by the the evaluator only considering the short term benefits of bringing the queen out (i.e. increased number of moves, controlling the centre). If there was more time to complete this project an increased penalty could have been added on moving the queen to early or some similar evaluation. It could also be suggested that if the evaluator searched to a greater depth it would realise the disadvantage of moving the queen out early. Again, this could have been done at a later stage in the project.

The second open ended question allowed testers to comment on the features they would like to see added to the programme. This meant that testers could give feedback on any part of the program but the majority was either about the GUI or more advanced features. For the GUI, seven of the users commented that they would wish to see a pop up box appear if the player is in check or checkmate, as many other chess games do. Others commented that a history of moves should be shown. Again this is a standard option for many computer chess games and Winboard could probably be expanded to do this. Further comments 
