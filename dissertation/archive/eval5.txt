Formal Evaluation of project

Once the product was completed it was time to a final evaluation in order to see if we had met our requirements set out in the original project specification which stated "The aim of this project is to develop a program that plays one of the more complex mind games at a standard good enough to beat casual players". \cite{Spec} To find out if the project realised these requirements we set out to do a formal evaluation process involving casual chess players. 

Following a briefing from a Team H member and after signing the ethics agreement (Found in Appendix A) the tester would select a difficulty and play a single game of chess against our AI. After completing the game the tester was asked to fill out a questionnaire (Appendix B). This questionnaire asks some brief questions about age and gender then goes on to discuss the program itself.

During the evaluation the chess AI won 8, lost 2 and drew 2 games. While on the face of it this may seem like a strong set of results for the AI and a solid achievement of our goal there is still many things to take into consideration. The first is the quality of the players. Of the twelve testers only five of them put their level above beginner, with all five claiming to be Amateur. Out of the three difficulty levels no person picked Challenging while the majority picked easy. Secondly, chess is a very difficult game to get people to play properly. When playing chess you really need no distractions and to spend some time considering moves. Some players, probably due to being watched by a member of the team felt stressed and did not think out moves properly. One such example of this was a player losing in seven moves to our AI. Obviously this would not have happened if the player was thinking about the moves properly. Also the amount of time consumed by a game would have been an issue. Some games would drag on and testers would lose interest and start making moves with the intent of losing. 

As part of the evaluation we asked three open ended questions to allow testers to give detailed feedback. Question one asked for further comments about the difficulty level. The majority of users left this blank however some users left insightful comments. One user complained that the engine needlessly threw away a pawn that was on the sixth rank, and did not retaliate. This same user then went on to win the game. Even at after a review of the moves it is not obvious why the engine did this as it could have became a queen at a later date. Another comment from several users was the queen coming out early in an attempt to quickly win the game. This often backfired against the more experienced players who spent developing pieces to push the queen back. We believe this was caused by the the evaluator only considering the short term benefits of bringing the queen out (i.e. increased number of moves, controlling the centre). If there was more time to complete this project an increased penalty could have been added on moving the queen to early or some similar evaluation. It could also be suggested that if the evaluator searched to a greater depth it would realise the disadvantage of moving the queen out early. Again, this could have been done at a later stage in the project.

The second open ended question allowed testers to comment on the features they would like to see added to the programme. This meant that testers could give feedback on any part of the program but the majority was either about the GUI or more advanced features. For the GUI, seven of the users commented that they would wish to see a pop up box appear if the player is in check or checkmate, as many other chess games do. Others commented that a history of moves should be shown. Again this is a standard option for many computer chess games and Winboard could probably be expanded to do this. Further suggested improvements on the UI were highlighting of invalid moves, changing or removing of the timer and a more obvious way of telling whose turn it is. There were also several suggestions not related UI. These looked at more technical feature that would improve the playability of the game. Among them were requests for a save function, an undo function, the ability to choose a difficulty on screen and start a new game straight after one has finished. Many of these features are actually included in Winboard, but have not been activated. 

The final open ended question was just asking for any further comments.The majority of testers left it blank and those he did comment said very nothing that has not already been covered in the above two sections. Other questions on the form were much more quantitative allowing users to pick from options. The main two questions for evaluation purposes were "How well did the game match the selected difficulty?" and "How easy was it to use the UI?". According to the results all users found that the game matched the selected difficulty but the UI could be improved slightly. 
